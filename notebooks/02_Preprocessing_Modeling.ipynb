{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043fc8fd",
   "metadata": {},
   "source": [
    "## Partie 2 – Traitement de texte (Text Preprocessing)\n",
    "\n",
    "# OBJECTIFS\n",
    "    - Nettoyer et transformer les tweets pour la modélisation\n",
    "    - Réduire la dimensionnalité du corpus\n",
    "    - Extraire une version épurée et utile du texte\n",
    "\n",
    "# Étapes à suivre\n",
    "\n",
    "- Écrire une fonction pour :\n",
    "    - Tokeniser un tweet\n",
    "    - Supprimer la ponctuation, les chiffres, les mots courts (< 3 lettres)\n",
    "    - Supprimer les stopwords\n",
    "    - Appliquer stemming ou lemmatisation\n",
    "- Reconstituer un corpus nettoyé\n",
    "- Analyser :\n",
    "    - Le nombre total de tokens\n",
    "    - Le nombre de tokens uniques\n",
    "    - Le nombre de tokens apparaissant une seule fois\n",
    "- Visualiser les tokens les plus fréquents (WordCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd28714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importer nos fonctions de preprocessing\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f340955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées avec succès: 7613 lignes, 5 colonnes\n",
      "Dataset: 7613 tweets, 5 colonnes\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "keyword",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ef1bbd9b-77e8-4fe0-b436-4edea71b0408",
       "rows": [
        [
         "0",
         "1",
         null,
         null,
         "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all",
         "1"
        ],
        [
         "1",
         "4",
         null,
         null,
         "Forest fire near La Ronge Sask. Canada",
         "1"
        ],
        [
         "2",
         "5",
         null,
         null,
         "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected",
         "1"
        ],
        [
         "3",
         "6",
         null,
         null,
         "13,000 people receive #wildfires evacuation orders in California ",
         "1"
        ],
        [
         "4",
         "7",
         null,
         null,
         "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ",
         "1"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des données\n",
    "df = load_data(\"../data/tweets.csv\")\n",
    "print(f\"Dataset: {df.shape[0]} tweets, {df.shape[1]} colonnes\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4111c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques des textes originaux:\n",
      "   total_tweets: 7613.00\n",
      "   avg_length: 101.04\n",
      "   median_length: 107.00\n",
      "   min_length: 7.00\n",
      "   max_length: 157.00\n",
      "   avg_word_count: 14.90\n",
      "   median_word_count: 15.00\n",
      "   min_word_count: 1.00\n",
      "   max_word_count: 31.00\n",
      "\n",
      "Outliers détectés:\n",
      "   Tweets trop courts (<10 char): 9 (0.1%)\n",
      "   Tweets trop longs (>280 char): 0 (0.0%)\n",
      "\n",
      "Exemples de tweets courts:\n",
      "   1. \"LOOOOOOL\"\n",
      "   2. \"Cooool :)\"\n",
      "   3. \"The end!\"\n",
      "\n",
      "Distribution des classes:\n",
      "   Classe 0 (non-catastrophe): 4342\n",
      "   Classe 1 (catastrophe): 3271\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Preprocessé: deed reason allah forgive all\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction complète de preprocessing d'un tweet selon les spécifications:\n",
    "# - Tokeniser\n",
    "# - Supprimer ponctuation, chiffres, mots courts (< 3 lettres)\n",
    "# - Supprimer stopwords\n",
    "# - Appliquer stemming/lemmatisation\n",
    "def preprocess_tweet(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # 1. Tokenisation et nettoyage initial\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Ponctuation et chiffres\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. Tokenisation\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 3. Supprimer mots courts (< 3 lettres)\n",
    "    tokens = [token for token in tokens if len(token) >= 3]\n",
    "    \n",
    "    # 4. Supprimer stopwords et appliquer stemming\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if not is_stopword(token):  # Fonction simple de détection de stopwords\n",
    "            stemmed = simple_stem(token)  # Fonction simple de stemming\n",
    "            processed_tokens.append(stemmed)\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "def is_stopword(word):\n",
    "    \"\"\"Fonction simple de détection des stopwords\"\"\"\n",
    "    stopwords = {\n",
    "        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',\n",
    "        'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after',\n",
    "        'above', 'below', 'between', 'among', 'through', 'during', 'before', 'after',\n",
    "        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do',\n",
    "        'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must',\n",
    "        'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "        'my', 'your', 'his', 'her', 'its', 'our', 'their', 'this', 'that', 'these', 'those'\n",
    "    }\n",
    "    return word.lower() in stopwords\n",
    "\n",
    "def simple_stem(word):\n",
    "    \"\"\"Fonction simple de stemming\"\"\"\n",
    "    if word.endswith('ing') and len(word) > 5:\n",
    "        return word[:-3]\n",
    "    elif word.endswith('ed') and len(word) > 4:\n",
    "        return word[:-2]\n",
    "    elif word.endswith('s') and len(word) > 3:\n",
    "        return word[:-1]\n",
    "    return word\n",
    "\n",
    "# Statistiques sur les textes originaux\n",
    "text_stats = get_text_statistics(df, 'text')\n",
    "print(\"Statistiques des textes originaux:\")\n",
    "for key, value in text_stats.items():\n",
    "    print(f\"   {key}: {value:.2f}\")\n",
    "\n",
    "# Identifier les outliers\n",
    "outliers_info = identify_outliers(df, 'text', min_length=10, max_length=280)\n",
    "print(f\"\\nOutliers détectés:\")\n",
    "print(f\"   Tweets trop courts (<10 char): {outliers_info['short_tweets_count']} ({outliers_info['short_tweets_percentage']:.1f}%)\")\n",
    "print(f\"   Tweets trop longs (>280 char): {outliers_info['long_tweets_count']} ({outliers_info['long_tweets_percentage']:.1f}%)\")\n",
    "\n",
    "# Exemples de tweets courts\n",
    "if outliers_info['short_tweets_examples']:\n",
    "    print(f\"\\nExemples de tweets courts:\")\n",
    "    for i, tweet in enumerate(outliers_info['short_tweets_examples'][:3]):\n",
    "        print(f\"   {i+1}. \\\"{tweet}\\\"\")\n",
    "\n",
    "# Distribution des classes\n",
    "if 'target' in df.columns:\n",
    "    print(f\"\\nDistribution des classes:\")\n",
    "    target_counts = df['target'].value_counts()\n",
    "    print(f\"   Classe 0 (non-catastrophe): {target_counts.get(0, 0)}\")\n",
    "    print(f\"   Classe 1 (catastrophe): {target_counts.get(1, 0)}\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df['target'].value_counts().plot(kind='bar', color=['skyblue', 'orange'])\n",
    "    plt.title('Distribution des classes')\n",
    "    plt.xlabel('Classe')\n",
    "    plt.ylabel('Nombre de tweets')\n",
    "    plt.xticks([0, 1], ['Non-catastrophe', 'Catastrophe'], rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "# Test de la fonction de preprocessing sur un échantillon\n",
    "sample_tweet = df['text'].iloc[0]\n",
    "print(f\"Original: {sample_tweet}\")\n",
    "print(f\"Preprocessé: {preprocess_tweet(sample_tweet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97dab395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Étape 1: Gestion des valeurs manquantes\n",
      "   Valeurs manquantes après traitement: 0\n",
      "\n",
      "Étape 2: Suppression des doublons\n",
      "Doublons supprimés: 0\n",
      "\n",
      "Étape 3: Exemples de nettoyage de texte\n",
      "\n",
      "Tweet 1:\n",
      "   Original: \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
      "   Nettoyé:  \"our deeds are the reason of this may allah forgive us all\"\n",
      "   Sans stopwords: \"deeds reason allah forgive\"\n",
      "   Avec stemming: \"deed reason allah forgive\"\n",
      "   Preprocessing complet: \"deed reason allah forgive\"\n",
      "\n",
      "Tweet 2:\n",
      "   Original: \"Forest fire near La Ronge Sask. Canada\"\n",
      "   Nettoyé:  \"forest fire near la ronge sask canada\"\n",
      "   Sans stopwords: \"forest fire near la ronge sask canada\"\n",
      "   Avec stemming: \"forest fire near la ronge sask canada\"\n",
      "   Preprocessing complet: \"forest fire near la ronge sask canada\"\n",
      "\n",
      "Tweet 3:\n",
      "   Original: \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      "   Nettoyé:  \"all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected\"\n",
      "   Sans stopwords: \"residents asked shelter place notified officers evacuation shelter place orders expected\"\n",
      "   Avec stemming: \"resident ask shelter place notify officer evacuation shelter place order expect\"\n",
      "   Preprocessing complet: \"resident ask shelter place notify officer evacuation shelter place order expect\"\n",
      "\n",
      "Étape 4: Application du preprocessing complet\n",
      "Preprocessing avec SpaCy...\n",
      "   Nouvelle colonne 'text_processed' créée\n",
      "   Shape après preprocessing: (7613, 6)\n",
      "\n",
      "Exemples de textes préprocessés:\n",
      "\n",
      "1. Original: \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all...\"\n",
      "   Preprocessé: \"deed reason allah forgive\"\n",
      "\n",
      "2. Original: \"Forest fire near La Ronge Sask. Canada...\"\n",
      "   Preprocessé: \"forest fire near la ronge sask canada\"\n",
      "\n",
      "3. Original: \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or she...\"\n",
      "   Preprocessé: \"resident ask shelter place notify officer evacuation shelter place order expect\"\n",
      "Preprocessing du corpus...\n",
      "Corpus original: 7613 tweets\n",
      "Corpus nettoyé: 7609 tweets\n",
      "Tweets supprimés: 4\n",
      "\n",
      "Exemples de transformation:\n",
      "\n",
      "1.\n",
      "Original: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all...\n",
      "Preprocessé: deed reason allah forgive all\n",
      "\n",
      "2.\n",
      "Original: Forest fire near La Ronge Sask. Canada...\n",
      "Preprocessé: forest fire near ronge sask canada\n",
      "\n",
      "3.\n",
      "Original: All residents asked to 'shelter in place' are being notified by officers. No oth...\n",
      "Preprocessé: all resident ask shelter place notifi officer other evacuation shelter place order expect\n",
      "   Nouvelle colonne 'text_processed' créée\n",
      "   Shape après preprocessing: (7613, 6)\n",
      "\n",
      "Exemples de textes préprocessés:\n",
      "\n",
      "1. Original: \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all...\"\n",
      "   Preprocessé: \"deed reason allah forgive\"\n",
      "\n",
      "2. Original: \"Forest fire near La Ronge Sask. Canada...\"\n",
      "   Preprocessé: \"forest fire near la ronge sask canada\"\n",
      "\n",
      "3. Original: \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or she...\"\n",
      "   Preprocessé: \"resident ask shelter place notify officer evacuation shelter place order expect\"\n",
      "Preprocessing du corpus...\n",
      "Corpus original: 7613 tweets\n",
      "Corpus nettoyé: 7609 tweets\n",
      "Tweets supprimés: 4\n",
      "\n",
      "Exemples de transformation:\n",
      "\n",
      "1.\n",
      "Original: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all...\n",
      "Preprocessé: deed reason allah forgive all\n",
      "\n",
      "2.\n",
      "Original: Forest fire near La Ronge Sask. Canada...\n",
      "Preprocessé: forest fire near ronge sask canada\n",
      "\n",
      "3.\n",
      "Original: All residents asked to 'shelter in place' are being notified by officers. No oth...\n",
      "Preprocessé: all resident ask shelter place notifi officer other evacuation shelter place order expect\n"
     ]
    }
   ],
   "source": [
    "# Étape 1: Gestion des valeurs manquantes\n",
    "print(\"Étape 1: Gestion des valeurs manquantes\")\n",
    "df_clean = handle_missing_values(df)\n",
    "print(f\"   Valeurs manquantes après traitement: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Étape 2: Suppression des doublons\n",
    "print(\"\\nÉtape 2: Suppression des doublons\")\n",
    "df_clean = remove_duplicates(df_clean)\n",
    "\n",
    "# Étape 3: Application du preprocessing à tout le dataset\n",
    "print(f\"\\nÉtape 3: Application du preprocessing complet\")\n",
    "df_processed = preprocess_text_column(df_clean, 'text')\n",
    "print(f\"   Nouvelle colonne 'text_processed' créée\")\n",
    "print(f\"   Shape après preprocessing: {df_processed.shape}\")\n",
    "\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b930c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE DU CORPUS:\n",
      "Nombre total de tokens: 65,350\n",
      "Nombre de tokens uniques: 11,195\n",
      "Tokens apparaissant une seule fois: 6,213\n",
      "Pourcentage de hapax: 55.5%\n",
      "Richesse lexicale: 0.171\n",
      "\n",
      "Top 20 des tokens les plus fréquents:\n",
      " 1. fire            (350 occurrences)\n",
      " 2. new             (349 occurrences)\n",
      " 3. like            (346 occurrences)\n",
      " 4. just            (320 occurrences)\n",
      " 5. not             (300 occurrences)\n",
      " 6. amp             (300 occurrences)\n",
      " 7. out             (271 occurrences)\n",
      " 8. all             (258 occurrences)\n",
      " 9. get             (255 occurrences)\n",
      "10. when            (255 occurrences)\n",
      "11. what            (227 occurrences)\n",
      "12. bomb            (222 occurrences)\n",
      "13. now             (221 occurrences)\n",
      "14. via             (220 occurrences)\n",
      "15. more            (217 occurrences)\n",
      "16. dont            (208 occurrences)\n",
      "17. one             (205 occurrences)\n",
      "18. people          (197 occurrences)\n",
      "19. there           (196 occurrences)\n",
      "20. how             (192 occurrences)\n",
      "\n",
      "Comparaison avant/après preprocessing:\n",
      "   avg_length: 101.0 → 50.4 (50.1% de réduction)\n",
      "   avg_word_count: 14.9 → 7.8 (47.6% de réduction)\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour tokeniser et analyser\n",
    "def analyze_corpus(texts):\n",
    "    \"\"\"Analyser un corpus de textes tokenisés\"\"\"\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        if pd.notna(text) and text.strip():\n",
    "            tokens = text.split()\n",
    "            # Filtrer les tokens courts (< 3 lettres)\n",
    "            tokens = [token for token in tokens if len(token) >= 3]\n",
    "            all_tokens.extend(tokens)\n",
    "    \n",
    "    return all_tokens\n",
    "\n",
    "# Analyse du corpus préprocessé\n",
    "\n",
    "# Extraction de tous les tokens du corpus préprocessé\n",
    "all_tokens = []\n",
    "for text in df_processed['text_processed']:\n",
    "    if pd.notna(text) and text.strip():\n",
    "        tokens = text.split()\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "# Statistiques des tokens\n",
    "total_tokens = len(all_tokens)\n",
    "unique_tokens = len(set(all_tokens))\n",
    "token_counts = Counter(all_tokens)\n",
    "hapax_tokens = sum(1 for count in token_counts.values() if count == 1)\n",
    "\n",
    "print(\"ANALYSE DU CORPUS PRÉPROCESSÉ:\")\n",
    "print(f\"Nombre total de tokens: {total_tokens:,}\")\n",
    "print(f\"Nombre de tokens uniques: {unique_tokens:,}\")\n",
    "print(f\"Tokens apparaissant une seule fois (hapax): {hapax_tokens:,}\")\n",
    "print(f\"Pourcentage de hapax: {hapax_tokens/unique_tokens*100:.1f}%\")\n",
    "print(f\"Richesse lexicale (TTR): {unique_tokens/total_tokens:.3f}\")\n",
    "\n",
    "# Top 20 des tokens les plus fréquents\n",
    "print(f\"\\nTop 20 des tokens les plus fréquents:\")\n",
    "for i, (token, count) in enumerate(token_counts.most_common(20), 1):\n",
    "    print(f\"{i:2d}. {token:<15} ({count:,} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b79761",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Créer des features supplémentaires\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_with_features = \u001b[43mcreate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeatures créées avec succès\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Afficher les nouvelles colonnes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\notebooks\\..\\src\\preprocessing.py:220\u001b[39m, in \u001b[36mcreate_features\u001b[39m\u001b[34m(df, text_column)\u001b[39m\n\u001b[32m    213\u001b[39m df_copy[\u001b[33m'\u001b[39m\u001b[33mexclamation_count\u001b[39m\u001b[33m'\u001b[39m] = df_copy[text_column].apply(\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x).count(\u001b[33m'\u001b[39m\u001b[33m!\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    215\u001b[39m )\n\u001b[32m    216\u001b[39m df_copy[\u001b[33m'\u001b[39m\u001b[33mquestion_count\u001b[39m\u001b[33m'\u001b[39m] = df_copy[text_column].apply(\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x).count(\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    218\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m df_copy[\u001b[33m'\u001b[39m\u001b[33mnamed_entities_count\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextract_named_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pos_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mNOUN\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVERB\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mADJ\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    225\u001b[39m     df_copy[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_type.lower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_count\u001b[39m\u001b[33m'\u001b[39m] = df_copy[text_column].apply(\n\u001b[32m    226\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: extract_pos_tags(\u001b[38;5;28mstr\u001b[39m(x)).get(pos_type, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    227\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\notebooks\\..\\src\\preprocessing.py:221\u001b[39m, in \u001b[36mcreate_features.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    213\u001b[39m df_copy[\u001b[33m'\u001b[39m\u001b[33mexclamation_count\u001b[39m\u001b[33m'\u001b[39m] = df_copy[text_column].apply(\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x).count(\u001b[33m'\u001b[39m\u001b[33m!\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    215\u001b[39m )\n\u001b[32m    216\u001b[39m df_copy[\u001b[33m'\u001b[39m\u001b[33mquestion_count\u001b[39m\u001b[33m'\u001b[39m] = df_copy[text_column].apply(\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x).count(\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    218\u001b[39m )\n\u001b[32m    220\u001b[39m df_copy[\u001b[33m'\u001b[39m\u001b[33mnamed_entities_count\u001b[39m\u001b[33m'\u001b[39m] = df_copy[text_column].apply(\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mextract_named_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    222\u001b[39m )\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pos_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mNOUN\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVERB\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mADJ\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    225\u001b[39m     df_copy[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_type.lower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_count\u001b[39m\u001b[33m'\u001b[39m] = df_copy[text_column].apply(\n\u001b[32m    226\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: extract_pos_tags(\u001b[38;5;28mstr\u001b[39m(x)).get(pos_type, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    227\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\notebooks\\..\\src\\preprocessing.py:378\u001b[39m, in \u001b[36mextract_named_entities\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.isna(text) \u001b[38;5;129;01mor\u001b[39;00m text == \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m entities = [ent.text \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc.ents]\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m entities\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\spacy\\pipeline\\tok2vec.py:121\u001b[39m, in \u001b[36mTok2Vec.predict\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m    119\u001b[39m     width = \u001b[38;5;28mself\u001b[39m.model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.model.ops.alloc((\u001b[32m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m tokvecs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m     76\u001b[39m Xf = layer.ops.flatten(Xs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m Yf, get_dXf = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\residual.py:41\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output + dX\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m Y, backprop_layer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] + Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "    \u001b[31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\layernorm.py:26\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     24\u001b[39m N, mu, var = _get_moments(model.ops, X)\n\u001b[32m     25\u001b[39m Xhat = (X - mu) * var ** (-\u001b[32m1.0\u001b[39m / \u001b[32m2.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m Y, backprop_rescale = \u001b[43m_begin_update_scale_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXhat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: InT) -> InT:\n\u001b[32m     29\u001b[39m     dY = backprop_rescale(dY)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Theo Boucebaine\\Documents\\GitHub\\tweet_project\\venv\\Lib\\site-packages\\thinc\\layers\\layernorm.py:58\u001b[39m, in \u001b[36m_begin_update_scale_shift\u001b[39m\u001b[34m(model, X)\u001b[39m\n\u001b[32m     54\u001b[39m     model.set_param(\u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m, model.ops.alloc1f(nI))\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_begin_update_scale_shift\u001b[39m(model: Model[InT, InT], X: InT) -> Tuple[InT, Callable]:\n\u001b[32m     59\u001b[39m     G = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mG\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m     b = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Créer des features supplémentaires\n",
    "df_with_features = create_features(df_processed, 'text')\n",
    "print(f\"Features créées avec succès\")\n",
    "\n",
    "# Afficher les nouvelles colonnes\n",
    "new_columns = [col for col in df_with_features.columns if col not in df_processed.columns]\n",
    "print(f\"Nouvelles features: {new_columns}\")\n",
    "\n",
    "# Statistiques des features\n",
    "print(f\"\\nStatistiques des features:\")\n",
    "feature_stats = df_with_features[new_columns].describe()\n",
    "display(feature_stats)\n",
    "\n",
    "# Corrélation entre features\n",
    "print(f\"\\nCorrélation entre les features numériques:\")\n",
    "numeric_features = df_with_features.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df_with_features[numeric_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Matrice de corrélation des features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Afficher quelques exemples avec features\n",
    "print(f\"\\nExemples avec features:\")\n",
    "sample_features = ['text', 'text_processed', 'text_length', 'word_count', 'hashtag_count', 'mention_count', 'url_count']\n",
    "available_features = [f for f in sample_features if f in df_with_features.columns]\n",
    "display(df_with_features[available_features].head())\n",
    "\n",
    "# Visualisation des tokens les plus fréquents\n",
    "\n",
    "# Création du WordCloud\n",
    "corpus_text = ' '.join(df_clean['text_processed'])\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# WordCloud global\n",
    "plt.subplot(1, 2, 1)\n",
    "wordcloud = WordCloud(width=600, height=400, \n",
    "                     background_color='white',\n",
    "                     max_words=100,\n",
    "                     colormap='viridis').generate(corpus_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud - Corpus complet')\n",
    "\n",
    "# WordCloud pour tweets de catastrophe (si colonne target existe)\n",
    "if 'target' in df_clean.columns:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    disaster_corpus = ' '.join(df_clean[df_clean['target'] == 1]['text_processed'])\n",
    "    wordcloud_disaster = WordCloud(width=600, height=400,\n",
    "                                  background_color='white',\n",
    "                                  max_words=100,\n",
    "                                  colormap='Reds').generate(disaster_corpus)\n",
    "    plt.imshow(wordcloud_disaster, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('WordCloud - Tweets de catastrophe')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution des fréquences des tokens\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "freq_dist = [count for token, count in token_counts.most_common(50)]\n",
    "plt.plot(range(1, 51), freq_dist)\n",
    "plt.title('Distribution des fréquences (Top 50)')\n",
    "plt.xlabel('Rang')\n",
    "plt.ylabel('Fréquence')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "freq_values = list(token_counts.values())\n",
    "plt.hist(freq_values, bins=50, alpha=0.7)\n",
    "plt.title('Histogramme des fréquences')\n",
    "plt.xlabel('Fréquence')\n",
    "plt.ylabel('Nombre de tokens')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse de la réduction de dimensionnalité\n",
    "original_tokens = []\n",
    "for text in df['text']:\n",
    "    if pd.notna(text):\n",
    "        tokens = text.lower().split()\n",
    "        original_tokens.extend(tokens)\n",
    "\n",
    "original_total = len(original_tokens)\n",
    "original_unique = len(set(original_tokens))\n",
    "\n",
    "print(\"RÉDUCTION DE DIMENSIONNALITÉ:\")\n",
    "print(f\"Avant: {original_total:,} tokens totaux, {original_unique:,} uniques\")\n",
    "print(f\"Après: {total_tokens:,} tokens totaux, {unique_tokens:,} uniques\")\n",
    "print(f\"Réduction: {(original_total - total_tokens)/original_total*100:.1f}% tokens, {(original_unique - unique_tokens)/original_unique*100:.1f}% vocabulaire\")\n",
    "\n",
    "# WordCloud simple\n",
    "corpus_text = ' '.join(df_clean['text_processed'])\n",
    "plt.figure(figsize=(10, 5))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=50).generate(corpus_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Tokens les plus fréquents')\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarde des données préprocessées\n",
    "df_clean.to_csv(\"../data/tweets_preprocessed.csv\", index=False)\n",
    "print(f\"Dataset preprocessé sauvegardé: {len(df_clean)} tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f2dfa",
   "metadata": {},
   "source": [
    "# PARTIE 3 - MODÉLISATION\n",
    "\n",
    "- Objectifs\n",
    "    - Construire un pipeline complet : vectorisation + modèle\n",
    "    - Évaluer les performances du modèle\n",
    "\n",
    "- Étapes à suivre\n",
    "    - Utiliser un TfidfVectorizer ou CountVectorizer et Word2vec\n",
    "    - Choisir un modèle de classification (Logistic Regression, SVM, etc.)\n",
    "    - Mettre en place un pipeline avec scikit-learn\n",
    "    - Utiliser train_test_split ou cross_val_score\n",
    "    - Évaluer avec : accuracy, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Préparation des données\n",
    "X = df_clean['text_processed']\n",
    "y = df_clean['target']\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train: {len(X_train)} échantillons\")\n",
    "print(f\"Test: {len(X_test)} échantillons\")\n",
    "print(f\"Distribution train: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Distribution test: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des pipelines de modèles\n",
    "models = {\n",
    "    'LogisticRegression_TF': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    'LogisticRegression_Count': Pipeline([\n",
    "        ('vectorizer', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    'SVM_TF': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(max_features=5000)),\n",
    "        ('classifier', SVC(kernel='linear', random_state=42))\n",
    "    ]),\n",
    "    'NaiveBayes_TF': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(max_features=5000)),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ]),\n",
    "    'RandomForest_TF': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(max_features=3000)),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Évaluation des modèles avec validation croisée\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
    "    results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    print(f\"{name}: F1-Score CV = {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "\n",
    "# Sélection du meilleur modèle\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['cv_mean'])\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nMeilleur modèle: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation du meilleur modèle\n",
    "if 'LogisticRegression' in best_model_name:\n",
    "    param_grid = {\n",
    "        'vectorizer__max_features': [3000, 5000, 8000],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "        'classifier__C': [0.1, 1, 10]\n",
    "    }\n",
    "elif 'SVM' in best_model_name:\n",
    "    param_grid = {\n",
    "        'vectorizer__max_features': [3000, 5000],\n",
    "        'classifier__C': [0.1, 1, 10]\n",
    "    }\n",
    "else:\n",
    "    param_grid = {\n",
    "        'vectorizer__max_features': [3000, 5000]\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(best_model, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Meilleurs paramètres: {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score F1 CV: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Modèle final optimisé\n",
    "final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation finale sur le test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Métriques de performance\n",
    "print(\"RÉSULTATS FINAUX:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-catastrophe', 'Catastrophe'],\n",
    "            yticklabels=['Non-catastrophe', 'Catastrophe'])\n",
    "plt.title('Matrice de confusion')\n",
    "plt.ylabel('Valeurs réelles')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.show()\n",
    "\n",
    "# Analyse des erreurs\n",
    "errors = X_test[y_test != y_pred]\n",
    "if len(errors) > 0:\n",
    "    print(f\"\\nExemples d'erreurs de classification:\")\n",
    "    for i, tweet in enumerate(errors.head(3)):\n",
    "        actual = y_test[errors.index[i]]\n",
    "        predicted = y_pred[errors.index[i]]\n",
    "        print(f\"{i+1}. Réel: {actual}, Prédit: {predicted}\")\n",
    "        print(f\"   Texte: {tweet[:100]}...\")\n",
    "\n",
    "print(f\"\\nModèle final sauvegardé: {best_model_name}\")\n",
    "print(f\"Précision sur test: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Nombre d'erreurs: {len(errors)}/{len(y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
